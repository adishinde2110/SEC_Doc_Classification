{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import pytesseract\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping to download the required pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8NO5wsE8x5E",
    "outputId": "27ceee40-7780-44b5-c495-96f193c64174"
   },
   "outputs": [],
   "source": [
    "# Define the base URL of the SEC folder containing PDFs\n",
    "base_url = \"https://www.sec.gov/Archives/edgar/vprr/\"\n",
    "\n",
    "# Define headers as per SEC's guidelines\n",
    "headers = {\n",
    "    \"User-Agent\": \"adishinde2110@gmail.com\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    \"Host\": \"www.sec.gov\"\n",
    "}\n",
    "\n",
    "# Define a function to download a PDF file\n",
    "def download_pdf(pdf_url, folder_name):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Create a directory if it doesn't exist\n",
    "        if not os.path.exists(folder_name):\n",
    "            os.makedirs(folder_name)\n",
    "\n",
    "        file_name = os.path.join(folder_name, pdf_url.split(\"/\")[-1])\n",
    "        with open(file_name, \"wb\") as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# List of parent directories\n",
    "parent_dirs = ['0004', '0006', '0009', '0010', '0011', '0100', '0101',\n",
    "               '0102', '0103','0104', '0105', '0106', '0107', '0108']\n",
    "\n",
    "# Create a folder called \"forms\" before creating the folder based on the last subdirectory of the URL\n",
    "base_folder_name = \"forms_raw\"\n",
    "\n",
    "# Loop through the parent directories\n",
    "for parent_dir in parent_dirs:\n",
    "    # Construct the full base URL for the current parent directory\n",
    "    current_base_url = base_url + parent_dir + \"/\"\n",
    "\n",
    "    # Send a request to the current base URL\n",
    "    response = requests.get(current_base_url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all links to PDF files in the folder\n",
    "    pdf_links = [link.get(\"href\") for link in soup.find_all(\"a\") if link.get(\"href\").endswith(\".pdf\")]\n",
    "\n",
    "    # Extract the last subdirectory of the URL to create a folder name\n",
    "    folder_name = current_base_url.rstrip(\"/\").split(\"/\")[-1]\n",
    "\n",
    "    # Combine the base folder name \"forms\" and the last subdirectory folder name\n",
    "    folder_name = os.path.join(base_folder_name, folder_name)\n",
    "\n",
    "    # Download the PDFs while respecting the request rate limit (10 requests per second)\n",
    "    for pdf_link in pdf_links:\n",
    "        pdf_url = current_base_url + pdf_link.rstrip(\"/\").split(\"/\")[-1]\n",
    "        download_pdf(pdf_url, folder_name)\n",
    "\n",
    "        # Pause to respect the request rate limit (10 requests per second)\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting pdf to image for OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JKeolwuBrpZT"
   },
   "outputs": [],
   "source": [
    "# Specify the root directory where your \"forms\" folder is located\n",
    "root_directory = './forms/'\n",
    "\n",
    "# Function to convert the first page of PDFs to images in a folder and delete the PDFs\n",
    "def convert_first_page_to_images_and_delete(folder_path):\n",
    "    # List all PDF files in the folder\n",
    "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(folder_path, pdf_file)\n",
    "        images = convert_from_path(pdf_path, first_page=0, last_page=1, poppler_path='C:/Program Files/poppler-23.08.0/Library/bin')\n",
    "\n",
    "        if images:\n",
    "            # Save the first page image (index 0) as a JPEG file\n",
    "            first_page_image = images[0]\n",
    "            image_path = os.path.join(folder_path, f'{pdf_file}_page1.jpg')\n",
    "            first_page_image.save(image_path, 'JPEG')\n",
    "\n",
    "        # Delete the PDF file\n",
    "        os.remove(pdf_path)\n",
    "\n",
    "# Iterate through subdirectories of the root directory\n",
    "for folder_name in os.listdir(root_directory):\n",
    "    folder_path = os.path.join(root_directory, folder_name)\n",
    "\n",
    "    if os.path.isdir(folder_path):\n",
    "        convert_first_page_to_images_and_delete(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating balanced dataset for all document types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_folder = './forms/'\n",
    "\n",
    "for folder in os.listdir(source_folder):\n",
    "    folder_path = os.path.join(source_folder, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        num_images = len(os.listdir(folder_path))\n",
    "        if num_images < 200:\n",
    "            images_to_duplicate = 200 - num_images\n",
    "            images_in_folder = os.listdir(folder_path)\n",
    "            for i in range(images_to_duplicate):\n",
    "                image_to_copy = images_in_folder[i % num_images]  # Cycle through existing images\n",
    "                source_image_path = os.path.join(folder_path, image_to_copy)\n",
    "                new_image_name = f'duplicated_{i}.jpg'\n",
    "                new_image_path = os.path.join(folder_path, new_image_name)\n",
    "                shutil.copy(source_image_path, new_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR and text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stopwords = [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
    "\n",
    "    # Tokenize the text by splitting it into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    \n",
    "    # Join the filtered words back into a single string\n",
    "    processed_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Step 1: Preprocess the images and create the dataset\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Specify the root directory containing subdirectories for each class\n",
    "root_dir = \"./forms/\"\n",
    "\n",
    "for folder_name in os.listdir(root_dir):\n",
    "    folder_path = os.path.join(root_dir, folder_name)\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Use OpenCV to load and preprocess the image\n",
    "        image = cv2.imread(file_path)\n",
    "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Use Tesseract OCR to extract text\n",
    "        extracted_text = pytesseract.image_to_string(grayscale_image)\n",
    "        \n",
    "        # Clean and preprocess the extracted text\n",
    "        cleaned_text = preprocess_text(extracted_text)\n",
    "\n",
    "        data.append(cleaned_text)\n",
    "        labels.append(folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF vectorizer and word ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1, securities, TF-IDF Score: 150.79066267028057\n",
      "Rank: 2, report, TF-IDF Score: 120.68742841136331\n",
      "Rank: 3, exchange, TF-IDF Score: 110.03243692359685\n",
      "Rank: 4, form, TF-IDF Score: 108.24084516057495\n",
      "Rank: 5, address, TF-IDF Score: 105.98944237295144\n",
      "Rank: 6, plan, TF-IDF Score: 98.06152678635914\n",
      "Rank: 7, registrant, TF-IDF Score: 86.82986475545856\n",
      "Rank: 8, transfer, TF-IDF Score: 86.59436681823345\n",
      "Rank: 9, accountant, TF-IDF Score: 86.48696796025334\n",
      "Rank: 10, notice, TF-IDF Score: 85.22003913933457\n",
      "Rank: 11, file, TF-IDF Score: 83.04552122006709\n",
      "Rank: 12, derivative, TF-IDF Score: 78.15262875703407\n",
      "Rank: 13, product, TF-IDF Score: 76.20767094130429\n",
      "Rank: 14, filing, TF-IDF Score: 73.98710064171973\n",
      "Rank: 15, manager, TF-IDF Score: 73.92702676352366\n",
      "Rank: 16, number, TF-IDF Score: 72.59059791312082\n",
      "Rank: 17, pursuant, TF-IDF Score: 70.69489533413363\n",
      "Rank: 18, no, TF-IDF Score: 70.51013813145148\n",
      "Rank: 19, agent, TF-IDF Score: 70.2200848160566\n",
      "Rank: 20, state, TF-IDF Score: 68.8839150453477\n",
      "Rank: 21, 2001, TF-IDF Score: 67.64821659077633\n",
      "Rank: 22, check, TF-IDF Score: 67.51242465495854\n",
      "Rank: 23, reporting, TF-IDF Score: 65.38849784562932\n",
      "Rank: 24, 1934, TF-IDF Score: 62.75195756526351\n",
      "Rank: 25, exemption, TF-IDF Score: 62.06375914699985\n",
      "Rank: 26, 13f, TF-IDF Score: 62.013018008465494\n",
      "Rank: 27, listing, TF-IDF Score: 61.51912279912523\n",
      "Rank: 28, initial, TF-IDF Score: 60.669251360432504\n",
      "Rank: 29, issuer, TF-IDF Score: 58.535156762861526\n",
      "Rank: 30, commission, TF-IDF Score: 58.042243155684694\n",
      "Rank: 31, public, TF-IDF Score: 56.63090406883201\n",
      "Rank: 32, holdings, TF-IDF Score: 56.115743949821656\n",
      "Rank: 33, annual, TF-IDF Score: 55.56533908455871\n",
      "Rank: 34, street, TF-IDF Score: 53.76276208575122\n",
      "Rank: 35, stock, TF-IDF Score: 53.45750465452728\n",
      "Rank: 36, 11, TF-IDF Score: 52.75986525797655\n",
      "Rank: 37, 15, TF-IDF Score: 52.21267160414574\n",
      "Rank: 38, code, TF-IDF Score: 50.866410550142575\n",
      "Rank: 39, amendment, TF-IDF Score: 49.24044439289777\n",
      "Rank: 40, citizens, TF-IDF Score: 49.03076339317957\n",
      "Rank: 41, transition, TF-IDF Score: 49.03076339317957\n",
      "Rank: 42, submissions, TF-IDF Score: 48.86075897807504\n",
      "Rank: 43, rule, TF-IDF Score: 48.22284675553665\n",
      "Rank: 44, name, TF-IDF Score: 48.014021190950174\n",
      "Rank: 45, year, TF-IDF Score: 47.78086580400533\n",
      "Rank: 46, fiscal, TF-IDF Score: 46.96613380500137\n",
      "Rank: 47, offering, TF-IDF Score: 46.72296805144472\n",
      "Rank: 48, reported, TF-IDF Score: 46.06399088689159\n",
      "Rank: 49, 82, TF-IDF Score: 45.967408203026274\n",
      "Rank: 50, financial, TF-IDF Score: 45.784052798736525\n",
      "Rank: 51, required, TF-IDF Score: 45.376041088664536\n",
      "Rank: 52, fee, TF-IDF Score: 45.06708723014217\n",
      "Rank: 53, states, TF-IDF Score: 44.346634710345775\n",
      "Rank: 54, executive, TF-IDF Score: 43.559458976982185\n",
      "Rank: 55, organization, TF-IDF Score: 43.224830080524015\n",
      "Rank: 56, washington, TF-IDF Score: 43.14077764252484\n",
      "Rank: 57, 20549, TF-IDF Score: 42.699767690821766\n",
      "Rank: 58, person, TF-IDF Score: 42.30735359023097\n",
      "Rank: 59, company, TF-IDF Score: 42.23355502033165\n",
      "Rank: 60, filed, TF-IDF Score: 41.745181922532176\n",
      "Rank: 61, 16, TF-IDF Score: 41.031489177739076\n",
      "Rank: 62, federal, TF-IDF Score: 40.902430458889306\n",
      "Rank: 63, 12g3, TF-IDF Score: 40.567045399364766\n",
      "Rank: 64, period, TF-IDF Score: 40.53866993782775\n",
      "Rank: 65, inc, TF-IDF Score: 40.21084850406508\n",
      "Rank: 66, one, TF-IDF Score: 40.16632232177449\n",
      "Rank: 67, by, TF-IDF Score: 39.48711727064547\n",
      "Rank: 68, estimated, TF-IDF Score: 39.3423260250167\n",
      "Rank: 69, signing, TF-IDF Score: 39.07537996304057\n",
      "Rank: 70, mark, TF-IDF Score: 38.587714968009365\n",
      "Rank: 71, complete, TF-IDF Score: 38.47711873946472\n",
      "Rank: 72, york, TF-IDF Score: 38.307240274705386\n",
      "Rank: 73, 2002, TF-IDF Score: 38.20898415439341\n",
      "Rank: 74, apr, TF-IDF Score: 38.090852263475206\n",
      "Rank: 75, business, TF-IDF Score: 37.5076883703255\n",
      "Rank: 76, 31, TF-IDF Score: 37.197093153017356\n",
      "Rank: 77, engaged, TF-IDF Score: 36.723750626863875\n",
      "Rank: 78, new, TF-IDF Score: 36.37989702514704\n",
      "Rank: 79, principal, TF-IDF Score: 36.103014985843416\n",
      "Rank: 80, service, TF-IDF Score: 35.91705142360203\n",
      "Rank: 81, def, TF-IDF Score: 35.414662767548144\n",
      "Rank: 82, proxy, TF-IDF Score: 35.414662767548144\n",
      "Rank: 83, suppl, TF-IDF Score: 35.414662767548144\n",
      "Rank: 84, other, TF-IDF Score: 35.3037609030957\n",
      "Rank: 85, processed, TF-IDF Score: 35.220997150125015\n",
      "Rank: 86, 17a, TF-IDF Score: 35.06535626113685\n",
      "Rank: 87, ta, TF-IDF Score: 34.84963013614134\n",
      "Rank: 88, contained, TF-IDF Score: 34.72397117196032\n",
      "Rank: 89, note, TF-IDF Score: 34.384252410096046\n",
      "Rank: 90, reinstatement, TF-IDF Score: 34.245544944316634\n",
      "Rank: 91, title, TF-IDF Score: 34.18806586340004\n",
      "Rank: 92, named, TF-IDF Score: 34.04227611966685\n",
      "Rank: 93, signed, TF-IDF Score: 33.86131339455237\n",
      "Rank: 94, city, TF-IDF Score: 33.749777230485144\n",
      "Rank: 95, office, TF-IDF Score: 33.50866868887969\n",
      "Rank: 96, duly, TF-IDF Score: 33.36649084927079\n",
      "Rank: 97, 12g32br, TF-IDF Score: 33.175252851881105\n",
      "Rank: 98, 2b, TF-IDF Score: 33.10133554435798\n",
      "Rank: 99, average, TF-IDF Score: 33.040187208932856\n",
      "Rank: 100, current, TF-IDF Score: 33.036269867107876\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create TF-IDF vectors\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features as needed\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# Get the feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the TF-IDF scores for each feature (word) across all documents\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0)\n",
    "\n",
    "# Create a dictionary that maps feature names to their TF-IDF scores\n",
    "feature_tfidf_scores = {}\n",
    "for feature, score in zip(feature_names, tfidf_scores.tolist()[0]):\n",
    "    feature_tfidf_scores[feature] = score\n",
    "\n",
    "# Sort the features by their TF-IDF scores in descending order\n",
    "sorted_features = sorted(feature_tfidf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get the top N most frequent words (features)\n",
    "top_n = 100  # You can adjust this to get the desired number of top words\n",
    "top_features = sorted_features[:top_n]\n",
    "\n",
    "# Print the top N most frequent words with word rank\n",
    "for rank, (feature, score) in enumerate(top_features, start=1):\n",
    "    print(f\"Rank: {rank}, {feature}, TF-IDF Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "82_Submissions_FS       1.00      1.00      1.00        32\n",
      "         Form_11K       1.00      1.00      1.00        47\n",
      "         Form_13F       1.00      1.00      1.00        48\n",
      "        Form_19B4       1.00      1.00      1.00        37\n",
      "          Form_6K       1.00      1.00      1.00        36\n",
      "           Form_D       1.00      1.00      1.00        32\n",
      "         Form_TA2       1.00      1.00      1.00        46\n",
      "       Form_X17A5       1.00      1.00      1.00        41\n",
      "           Others       1.00      1.00      1.00        41\n",
      "\n",
      "         accuracy                           1.00       360\n",
      "        macro avg       1.00      1.00      1.00       360\n",
      "     weighted avg       1.00      1.00      1.00       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Train a classification model (e.g., SVM)\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
